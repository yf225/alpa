from typing import Sequence
import functools

import torch
import numpy as np
import jax
from alpa.device_mesh import DistributedArray, ReplicatedDistributedArray
from alpa.mesh_executable import create_remote_buffer_refs


# Copied from https://github.com/pytorch/pytorch/blob/ac79c874cefee2f8bc1605eed9a924d80c0b3542/torch/testing/_internal/common_utils.py#L349
# Dict of NumPy dtype -> torch dtype (when the correspondence exists)
numpy_to_torch_dtype_dict = {
    np.dtype(np.bool)       : torch.bool,
    np.dtype(np.uint8)      : torch.uint8,
    np.dtype(np.int8)       : torch.int8,
    np.dtype(np.int16)      : torch.int16,
    np.dtype(np.int32)      : torch.int32,
    np.dtype(np.int64)      : torch.int64,
    np.dtype(np.float16)    : torch.float16,
    np.dtype(np.float32)    : torch.float32,
    np.dtype(np.float64)    : torch.float64,
    np.dtype(np.complex64)  : torch.complex64,
    np.dtype(np.complex128) : torch.complex128
}

# Dict of torch dtype -> NumPy dtype
torch_to_numpy_dtype_dict = {value : key for (key, value) in numpy_to_torch_dtype_dict.items()}

def make_shaped_array_from_pt_tensor(pt_tensor):
    shape = list(pt_tensor.shape)
    np_dtype = torch_to_numpy_dtype_dict[pt_tensor.dtype]
    return jax.abstract_arrays.ShapedArray(shape, np_dtype)


def init_buffer(init_func,
                init_func_kwargs,
                local_rng_seed,
                worker,
                uuid: int,
                device_id: int,
                shape: Sequence[int],
                dtype=np.dtype(np.float32)):

    torch_local_rng = torch.Generator()
    torch_local_rng.manual_seed(local_rng_seed)
    init_func_kwargs["rng"] = torch_local_rng
    init_func_kwargs["shape"] = shape
    init_func_kwargs["dtype"] = numpy_to_torch_dtype_dict[dtype]

    worker.buffers[uuid] = (worker.backend.buffer_from_pyval(
        init_func(**init_func_kwargs), worker.local_devices[device_id]))


def array_init_like(array, init_func, **kwargs):
    # NOTE: Only torch operations are supported in `init_func`

    # This ensures that local RNG on all hosts share the same initial seed generated by global RNG.
    local_rng_seed = torch.randint(2147483647, (1,), generator=torch.default_generator).item()

    ret = None
    if isinstance(array, DistributedArray):
        num_batch = 1
        device_mesh = array.device_mesh
        indices = array.indices
        buf_refs, buf_uuids = create_remote_buffer_refs(device_mesh, num_batch)
        step = device_mesh.num_devices_per_host * num_batch
        for host_id in range(device_mesh.num_hosts):
            device_mesh.workers[host_id].shard_and_apply_func_on_buffer.remote(
                buf_uuids[host_id * step:(host_id + 1) * step], array._value.shape,
                array._value.dtype, indices[host_id * step:(host_id + 1) * step],
                num_batch, functools.partial(init_buffer, init_func, kwargs, local_rng_seed))
        ret = DistributedArray(device_mesh, array.aval, array.sharding_spec, buf_refs,
                                indices)
    elif isinstance(array, ReplicatedDistributedArray):
        device_meshes = []
        distributed_arrays = []
        for device_mesh, dist_array in array._mesh_array_map.items():
            meshes.append(device_mesh)
            distributed_arrays.append(array_init_like(dist_array, init_func, **kwargs))
        ret = ReplicatedDistributedArray(device_meshes, distributed_arrays)

    if ret is not None:
        return ret
    else:
        raise NotImplementedError(f"init_like does not support array of type {type(x)}")